100000
200000
300000
400000
500000
600000
700000
800000
900000
1000000
1100000
1200000
1300000
1400000
1500000
1600000
1700000
1800000
1900000
wordVectorsDic length 1917494
number of documents is  11109
self.V=all the words in all documents
8110
SampleNo:1
result/

---------RUN-------- 0
K0iterNum5SampleNum1alpha0.03beta0.03BatchNum16BatchSaved16
No timefil!
before call run_MStream, self.K=0,self.iterNum=5, self.V=8110
batchNum2tweetID is  {1: 8372, 2: 2346, 3: 10882, 4: 3650, 5: 2924, 6: 7530, 7: 10808, 8: 2856, 9: 4395, 10: 3787, 11: 5551, 12: 2133, 13: 955, 14: 5924, 15: 4084, 16: -1}
Batch 1
11109
	695 documents will be analyze. alpha is 20.85.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=220
outlier=29, non-outlier=666,=maxPredLabel=248
high_entropy_words=144, total words=915
outsPerCluster.values(), nonOuts 29 666
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 10, 5, 28, 49, 28, 6, 4, 3, 9, 9, 2, 12, 3, 4]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
oldPredLabel not cluster=old206, cluster=496
oldPredLabel not cluster=old209, cluster=499
Evaluate 2
evaluate total texts=695
homogeneity_score-whole-data:   0.97480147
completeness_score-whole-data:   0.53478100
nmi_score-whole-data:   0.69066158
pred clusters=55, true clusters=8
purity majority whole data=0.9899280575539569
	Gibbs sampling successful! Start to saving results.
start doc=0, end doc=695
total texts=695, total clusters=57
	Saving successful!
Batch 2
11109
	1391 documents will be analyze. alpha is 41.73.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=643
to delete batch 3 2
dict_keys([2, 3])
targetbatchNo 2
len(docsPerBatch) 695
outlier=18, non-outlier=678,=maxPredLabel=669
high_entropy_words=170, total words=1078
outsPerCluster.values(), nonOuts 18 1344
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 14, 3, 4, 52, 21, 7, 61, 68, 8, 39, 10, 115, 2, 61, 70, 54, 19, 14, 6, 6, 3, 3, 5, 4, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 3
evaluate total texts=696
homogeneity_score-whole-data:   0.97639205
completeness_score-whole-data:   0.62541051
nmi_score-whole-data:   0.76244834
pred clusters=41, true clusters=10
purity majority whole data=0.9841954022988506
	Gibbs sampling successful! Start to saving results.
start doc=695, end doc=1391
total texts=1391, total clusters=96
	Saving successful!
Batch 3
11109
	2087 documents will be analyze. alpha is 62.61.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=807
to delete batch 4 3
dict_keys([2, 3, 4])
targetbatchNo 3
len(docsPerBatch) 696
outlier=22, non-outlier=674,=maxPredLabel=824
high_entropy_words=149, total words=1079
outsPerCluster.values(), nonOuts 22 2018
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 61, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 18, 28]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 4
evaluate total texts=696
homogeneity_score-whole-data:   0.99586530
completeness_score-whole-data:   0.82926960
nmi_score-whole-data:   0.90496414
pred clusters=37, true clusters=12
purity majority whole data=0.9971264367816092
	Gibbs sampling successful! Start to saving results.
start doc=1391, end doc=2087
total texts=2087, total clusters=122
	Saving successful!
Batch 4
11109
	2783 documents will be analyze. alpha is 83.49.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=965
to delete batch 5 4
dict_keys([2, 3, 4, 5])
targetbatchNo 4
len(docsPerBatch) 696
outlier=22, non-outlier=674,=maxPredLabel=980
high_entropy_words=134, total words=958
outsPerCluster.values(), nonOuts 22 2692
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 75, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 21, 323, 101, 57, 63, 6, 101, 15, 2, 4, 2, 5, 4, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 5
evaluate total texts=696
homogeneity_score-whole-data:   0.85166979
completeness_score-whole-data:   0.79032621
nmi_score-whole-data:   0.81985213
pred clusters=35, true clusters=15
purity majority whole data=0.9109195402298851
	Gibbs sampling successful! Start to saving results.
start doc=2087, end doc=2783
total texts=2783, total clusters=149
	Saving successful!
Batch 5
11109
	3479 documents will be analyze. alpha is 104.37.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1134
to delete batch 6 5
dict_keys([2, 3, 4, 5, 6])
targetbatchNo 5
len(docsPerBatch) 696
outlier=28, non-outlier=668,=maxPredLabel=1161
high_entropy_words=179, total words=1114
outsPerCluster.values(), nonOuts 28 3360
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 412, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 41, 42, 64, 56, 15, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 6
evaluate total texts=696
homogeneity_score-whole-data:   0.95275964
completeness_score-whole-data:   0.78436258
nmi_score-whole-data:   0.86039888
pred clusters=54, true clusters=18
purity majority whole data=0.9727011494252874
	Gibbs sampling successful! Start to saving results.
start doc=2783, end doc=3479
total texts=3479, total clusters=180
	Saving successful!
Batch 6
11109
	4175 documents will be analyze. alpha is 125.25.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1230
to delete batch 7 6
dict_keys([2, 3, 4, 5, 6, 7])
targetbatchNo 6
len(docsPerBatch) 696
outlier=11, non-outlier=685,=maxPredLabel=1242
high_entropy_words=147, total words=906
outsPerCluster.values(), nonOuts 11 4045
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 424, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 218, 44, 64, 200, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 135, 89, 45, 29, 4, 2, 6]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 7
evaluate total texts=696
homogeneity_score-whole-data:   0.76652978
completeness_score-whole-data:   0.58364552
nmi_score-whole-data:   0.66270161
pred clusters=22, true clusters=11
purity majority whole data=0.8505747126436781
	Gibbs sampling successful! Start to saving results.
start doc=3479, end doc=4175
total texts=4175, total clusters=195
	Saving successful!
Batch 7
11109
	4871 documents will be analyze. alpha is 146.13.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1316
to delete batch 8 7
dict_keys([2, 3, 4, 5, 6, 7, 8])
targetbatchNo 7
len(docsPerBatch) 696
outlier=13, non-outlier=683,=maxPredLabel=1341
high_entropy_words=157, total words=997
outsPerCluster.values(), nonOuts 13 4728
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 71, 30, 6, 15, 4, 15, 35, 17, 2, 2, 3, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 8
evaluate total texts=696
homogeneity_score-whole-data:   0.93139813
completeness_score-whole-data:   0.80365476
nmi_score-whole-data:   0.86282389
pred clusters=37, true clusters=18
purity majority whole data=0.9324712643678161
	Gibbs sampling successful! Start to saving results.
start doc=4175, end doc=4871
total texts=4871, total clusters=221
	Saving successful!
Batch 8
11109
	5567 documents will be analyze. alpha is 167.01.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1419
to delete batch 9 8
dict_keys([2, 3, 4, 5, 6, 7, 8, 9])
targetbatchNo 8
len(docsPerBatch) 696
outlier=12, non-outlier=684,=maxPredLabel=1432
high_entropy_words=96, total words=881
outsPerCluster.values(), nonOuts 12 5412
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 92, 74, 58, 2, 10, 3, 4, 3]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 9
evaluate total texts=696
homogeneity_score-whole-data:   0.98691481
completeness_score-whole-data:   0.89046484
nmi_score-whole-data:   0.93621228
pred clusters=25, true clusters=11
purity majority whole data=0.9956896551724138
	Gibbs sampling successful! Start to saving results.
start doc=4871, end doc=5567
total texts=5567, total clusters=239
	Saving successful!
Batch 9
11109
	6263 documents will be analyze. alpha is 187.89.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1516
to delete batch 10 9
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10])
targetbatchNo 9
len(docsPerBatch) 696
outlier=13, non-outlier=683,=maxPredLabel=1527
high_entropy_words=93, total words=855
outsPerCluster.values(), nonOuts 13 6095
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 174, 74, 104, 2, 10, 3, 236, 161, 20, 110, 2, 6, 14, 11, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 10
evaluate total texts=696
homogeneity_score-whole-data:   0.96136900
completeness_score-whole-data:   0.86256175
nmi_score-whole-data:   0.90928905
pred clusters=23, true clusters=9
purity majority whole data=0.9841954022988506
	Gibbs sampling successful! Start to saving results.
start doc=5567, end doc=6263
total texts=6263, total clusters=255
	Saving successful!
Batch 10
11109
	6959 documents will be analyze. alpha is 208.77.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1687
to delete batch 11 10
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
targetbatchNo 10
len(docsPerBatch) 696
outlier=24, non-outlier=672,=maxPredLabel=1702
high_entropy_words=155, total words=1087
outsPerCluster.values(), nonOuts 24 6767
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 104, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 244, 161, 30, 110, 2, 6, 14, 216, 101, 35, 3, 113, 38, 26, 33, 13, 54, 31]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 11
evaluate total texts=696
homogeneity_score-whole-data:   0.98139396
completeness_score-whole-data:   0.67513941
nmi_score-whole-data:   0.79995701
pred clusters=39, true clusters=13
purity majority whole data=0.9913793103448276
	Gibbs sampling successful! Start to saving results.
start doc=6263, end doc=6959
total texts=6959, total clusters=285
	Saving successful!
Batch 11
11109
	7655 documents will be analyze. alpha is 229.65.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1864
to delete batch 12 11
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])
targetbatchNo 11
len(docsPerBatch) 696
outlier=23, non-outlier=673,=maxPredLabel=1892
high_entropy_words=94, total words=1099
outsPerCluster.values(), nonOuts 23 7440
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 106, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 69, 54, 31, 38, 77, 66, 7, 15, 31, 9, 97, 36, 7, 40, 5, 20, 2, 2, 8, 13, 27, 7, 15, 4, 10, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 12
evaluate total texts=696
homogeneity_score-whole-data:   0.90696013
completeness_score-whole-data:   0.67701016
nmi_score-whole-data:   0.77529386
pred clusters=51, true clusters=15
purity majority whole data=0.9468390804597702
	Gibbs sampling successful! Start to saving results.
start doc=6959, end doc=7655
total texts=7655, total clusters=320
	Saving successful!
Batch 12
11109
	8351 documents will be analyze. alpha is 250.53.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2018
to delete batch 13 12
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])
targetbatchNo 12
len(docsPerBatch) 696
outlier=33, non-outlier=663,=maxPredLabel=2046
high_entropy_words=174, total words=1032
outsPerCluster.values(), nonOuts 33 8103
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 108, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 82, 54, 31, 39, 79, 67, 7, 15, 150, 15, 99, 63, 7, 40, 5, 20, 2, 2, 8, 95, 27, 7, 102, 4, 14, 142, 103, 16, 34, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 13
evaluate total texts=696
homogeneity_score-whole-data:   0.95132059
completeness_score-whole-data:   0.59222460
nmi_score-whole-data:   0.73000189
pred clusters=57, true clusters=12
purity majority whole data=0.9770114942528736
	Gibbs sampling successful! Start to saving results.
start doc=7655, end doc=8351
total texts=8351, total clusters=351
	Saving successful!
Batch 13
11109
	9047 documents will be analyze. alpha is 271.41.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2279
to delete batch 14 13
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])
targetbatchNo 13
len(docsPerBatch) 696
outlier=40, non-outlier=656,=maxPredLabel=2301
high_entropy_words=146, total words=984
outsPerCluster.values(), nonOuts 40 8759
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 108, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 426, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 82, 54, 31, 39, 79, 258, 7, 15, 150, 15, 99, 73, 7, 40, 5, 20, 2, 2, 8, 95, 27, 7, 106, 4, 14, 142, 168, 16, 34, 2, 3, 2, 2, 2, 2, 50, 8, 68, 48, 59, 92, 22, 13, 2, 19, 2, 2, 2, 2, 2, 2, 4]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 14
evaluate total texts=696
homogeneity_score-whole-data:   0.88999825
completeness_score-whole-data:   0.70285158
nmi_score-whole-data:   0.78543082
pred clusters=59, true clusters=19
purity majority whole data=0.9209770114942529
	Gibbs sampling successful! Start to saving results.
start doc=8351, end doc=9047
total texts=9047, total clusters=395
	Saving successful!
Batch 14
11109
	9743 documents will be analyze. alpha is 292.29.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2562
to delete batch 15 14
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
targetbatchNo 14
len(docsPerBatch) 696
outlier=32, non-outlier=664,=maxPredLabel=2596
high_entropy_words=173, total words=1122
outsPerCluster.values(), nonOuts 32 9423
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 108, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 428, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 82, 54, 31, 39, 79, 258, 7, 15, 150, 15, 99, 90, 7, 40, 5, 20, 2, 2, 8, 95, 27, 7, 106, 4, 14, 142, 168, 16, 34, 2, 3, 2, 2, 2, 2, 50, 8, 68, 48, 118, 92, 22, 13, 2, 19, 2, 2, 2, 2, 2, 81, 43, 10, 33, 21, 49, 28, 7, 29, 48, 32, 15, 4, 8, 21, 18, 18, 9, 26, 15, 16, 26, 2, 3, 4, 8, 9, 2, 2, 3, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 15
evaluate total texts=696
homogeneity_score-whole-data:   0.97304657
completeness_score-whole-data:   0.93172154
nmi_score-whole-data:   0.95193577
pred clusters=65, true clusters=38
purity majority whole data=0.9698275862068966
	Gibbs sampling successful! Start to saving results.
start doc=9047, end doc=9743
total texts=9743, total clusters=446
	Saving successful!
Batch 15
11109
	10439 documents will be analyze. alpha is 313.17.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2801
to delete batch 16 15
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])
targetbatchNo 15
len(docsPerBatch) 696
outlier=37, non-outlier=659,=maxPredLabel=2838
high_entropy_words=189, total words=1217
outsPerCluster.values(), nonOuts 37 10082
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 108, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 32, 164, 110, 31, 7, 14, 23, 564, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 82, 54, 31, 39, 79, 258, 7, 15, 150, 15, 99, 90, 7, 40, 5, 20, 2, 2, 8, 95, 27, 7, 106, 4, 14, 142, 168, 16, 34, 2, 3, 2, 2, 2, 2, 50, 8, 68, 48, 123, 92, 22, 13, 2, 19, 2, 2, 2, 2, 2, 81, 43, 10, 33, 23, 49, 28, 7, 58, 48, 34, 15, 4, 8, 24, 18, 65, 9, 26, 16, 16, 26, 2, 3, 68, 8, 36, 2, 27, 83, 41, 3, 17, 2, 18, 14, 4, 20, 2, 13, 2, 19, 9, 4, 24, 4, 6, 8, 2, 4, 4, 8, 2, 2, 2, 2, 4]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 16
evaluate total texts=696
homogeneity_score-whole-data:   0.93879528
completeness_score-whole-data:   0.74073495
nmi_score-whole-data:   0.82808688
pred clusters=73, true clusters=24
purity majority whole data=0.9612068965517241
	Gibbs sampling successful! Start to saving results.
start doc=9743, end doc=10439
total texts=10439, total clusters=492
	Saving successful!
Batch 16
11109
	11109 documents will be analyze. alpha is 333.27.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2932
to delete batch 17 16
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])
targetbatchNo 16
len(docsPerBatch) 696
outlier=16, non-outlier=654,=maxPredLabel=2950
high_entropy_words=132, total words=1028
outsPerCluster.values(), nonOuts 16 10736
[17, 38, 64, 25, 17, 157, 9, 8, 9, 61, 12, 27, 24, 26, 18, 5, 28, 49, 28, 35, 4, 12, 9, 9, 2, 20, 3, 4, 52, 21, 7, 61, 70, 10, 40, 10, 115, 2, 110, 71, 54, 19, 77, 8, 6, 3, 3, 5, 4, 104, 91, 90, 164, 110, 31, 7, 14, 23, 690, 101, 59, 64, 6, 175, 15, 2, 4, 2, 5, 22, 2, 52, 74, 37, 273, 173, 64, 201, 55, 2, 4, 25, 3, 18, 6, 2, 5, 3, 2, 2, 170, 89, 97, 133, 4, 6, 6, 8, 9, 68, 12, 101, 30, 6, 15, 4, 15, 35, 17, 2, 2, 208, 55, 142, 10, 175, 74, 107, 2, 10, 3, 250, 161, 30, 110, 2, 6, 14, 271, 101, 51, 3, 113, 38, 26, 33, 82, 54, 31, 39, 79, 258, 7, 15, 150, 15, 99, 90, 7, 40, 5, 20, 2, 2, 8, 95, 27, 7, 106, 4, 14, 142, 168, 16, 34, 2, 3, 2, 2, 2, 2, 50, 8, 68, 48, 123, 92, 22, 13, 2, 19, 2, 2, 2, 2, 2, 81, 43, 10, 33, 23, 49, 28, 7, 58, 48, 138, 15, 4, 8, 24, 18, 65, 9, 26, 16, 16, 26, 2, 3, 68, 8, 36, 2, 27, 83, 43, 3, 17, 2, 18, 14, 4, 20, 2, 13, 2, 19, 9, 4, 24, 4, 6, 8, 2, 4, 4, 8, 2, 2, 2, 50, 74, 30, 66, 27, 32, 23, 38, 14, 2, 2, 5, 5]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 17
evaluate total texts=670
homogeneity_score-whole-data:   0.89454946
completeness_score-whole-data:   0.85323945
nmi_score-whole-data:   0.87340626
pred clusters=32, true clusters=14
purity majority whole data=0.891044776119403
	Gibbs sampling successful! Start to saving results.
start doc=10439, end doc=11109
total texts=11109, total clusters=515
	Saving successful!
time diff secs= 161
pred_true_text_file name=result/mstr-enh
evaluate total texts=11109
homogeneity_score-whole-data:   0.92360149
completeness_score-whole-data:   0.85708276
nmi_score-whole-data:   0.88909970
pred clusters=556, true clusters=152
purity majority whole data=0.8642542082995769
pred_true_text_file name=result/NewsPredTueTextMStream_WordArr.txt
evaluate total texts=11109
homogeneity_score-whole-data:   0.92137274
completeness_score-whole-data:   0.86030660
nmi_score-whole-data:   0.88979316
pred clusters=490, true clusters=152
purity majority whole data=0.8622738320280854
