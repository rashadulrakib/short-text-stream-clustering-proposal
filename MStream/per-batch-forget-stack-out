100000
200000
300000
400000
500000
600000
700000
800000
900000
1000000
1100000
1200000
1300000
1400000
1500000
1600000
1700000
1800000
1900000
wordVectorsDic length 1917494
number of documents is  20000
self.V=all the words in all documents
10275
SampleNo:1
result/

---------RUN-------- 0
K0iterNum5SampleNum1alpha0.03beta0.03BatchNum16BatchSaved16
No timefil!
before call run_MStream, self.K=0,self.iterNum=5, self.V=10275
batchNum2tweetID is  {1: 1251, 2: 2502, 3: 3753, 4: 5004, 5: 6255, 6: 7506, 7: 8757, 8: 10008, 9: 11259, 10: 12510, 11: 13761, 12: 15012, 13: 16263, 14: 17514, 15: 18765, 16: -1}
Batch 1
20000
	1250 documents will be analyze. alpha is 37.50.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=503
outlier=39, non-outlier=1211,=maxPredLabel=584
high_entropy_words=256, total words=1925
outsPerCluster.values(), nonOuts 39 1211
[9, 24, 20, 45, 28, 25, 14, 9, 57, 11, 20, 34, 7, 13, 2, 6, 27, 20, 24, 15, 16, 19, 57, 12, 17, 22, 19, 22, 18, 13, 2, 7, 47, 34, 15, 21, 6, 51, 24, 15, 12, 15, 4, 8, 47, 18, 27, 9, 2, 8, 6, 5, 2, 4, 28, 24, 5, 37, 6, 3, 6, 9, 14, 4, 5, 2, 2, 2, 2, 3, 4, 2, 3, 14, 6, 2, 2, 5, 3, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
oldPredLabel not cluster=old496, cluster=886
Evaluate 2
evaluate total texts=1250
homogeneity_score-whole-data:   0.66858126
completeness_score-whole-data:   0.39308146
nmi_score-whole-data:   0.49508548
pred clusters=119, true clusters=20
purity majority whole data=0.7192
	Gibbs sampling successful! Start to saving results.
start doc=0, end doc=1250
total texts=1250, total clusters=120
	Saving successful!
Batch 2
20000
	2501 documents will be analyze. alpha is 75.03.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1121
to delete batch 3 2
dict_keys([2, 3])
targetbatchNo 2
len(docsPerBatch) 1250
outlier=55, non-outlier=1196,=maxPredLabel=1200
high_entropy_words=211, total words=1940
outsPerCluster.values(), nonOuts 55 2407
[22, 60, 28, 65, 54, 58, 30, 15, 108, 14, 39, 63, 14, 29, 2, 8, 41, 60, 40, 17, 39, 43, 88, 15, 35, 39, 34, 51, 31, 33, 2, 8, 81, 98, 42, 31, 14, 118, 65, 50, 24, 26, 5, 21, 77, 32, 64, 14, 2, 29, 7, 15, 2, 6, 49, 53, 5, 50, 7, 8, 12, 21, 23, 9, 11, 2, 2, 2, 2, 3, 6, 2, 3, 30, 17, 2, 2, 13, 24, 2, 2, 2, 2, 4, 2, 2, 2, 12, 2, 2, 2, 2, 3]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 3
evaluate total texts=1251
homogeneity_score-whole-data:   0.69034184
completeness_score-whole-data:   0.44132436
nmi_score-whole-data:   0.53843558
pred clusters=131, true clusters=20
purity majority whole data=0.7082334132693845
	Gibbs sampling successful! Start to saving results.
start doc=1250, end doc=2501
total texts=2501, total clusters=160
	Saving successful!
Batch 3
20000
	3752 documents will be analyze. alpha is 112.56.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1396
to delete batch 4 3
dict_keys([2, 3, 4])
targetbatchNo 3
len(docsPerBatch) 1251
outlier=63, non-outlier=1188,=maxPredLabel=1454
high_entropy_words=264, total words=1992
outsPerCluster.values(), nonOuts 63 3595
[32, 90, 33, 93, 77, 96, 32, 17, 162, 14, 64, 91, 27, 46, 2, 8, 48, 112, 59, 17, 50, 71, 132, 18, 59, 62, 41, 85, 39, 42, 2, 8, 121, 144, 69, 35, 24, 217, 91, 71, 28, 34, 5, 26, 106, 47, 123, 14, 2, 53, 7, 21, 2, 6, 83, 77, 5, 55, 7, 8, 23, 38, 35, 9, 13, 2, 2, 2, 2, 3, 10, 2, 3, 52, 27, 2, 2, 27, 43, 2, 2, 2, 7, 4, 2, 2, 2, 27, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 11, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 4
evaluate total texts=1251
homogeneity_score-whole-data:   0.62862158
completeness_score-whole-data:   0.43067086
nmi_score-whole-data:   0.51115063
pred clusters=122, true clusters=20
purity majority whole data=0.6474820143884892
	Gibbs sampling successful! Start to saving results.
start doc=2501, end doc=3752
total texts=3752, total clusters=203
	Saving successful!
Batch 4
20000
	5003 documents will be analyze. alpha is 150.09.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1723
to delete batch 5 4
dict_keys([2, 3, 4, 5])
targetbatchNo 4
len(docsPerBatch) 1251
outlier=82, non-outlier=1169,=maxPredLabel=1775
high_entropy_words=287, total words=1990
outsPerCluster.values(), nonOuts 82 4764
[37, 123, 33, 130, 103, 191, 32, 17, 223, 14, 89, 119, 40, 56, 2, 8, 50, 168, 70, 17, 56, 95, 192, 18, 74, 94, 46, 109, 43, 51, 2, 8, 151, 220, 106, 35, 37, 317, 120, 75, 30, 42, 5, 26, 128, 60, 182, 16, 2, 74, 7, 21, 2, 6, 102, 111, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 14, 2, 3, 55, 39, 2, 2, 52, 63, 2, 2, 2, 9, 4, 2, 2, 2, 39, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 5
evaluate total texts=1251
homogeneity_score-whole-data:   0.60008379
completeness_score-whole-data:   0.42295669
nmi_score-whole-data:   0.49618654
pred clusters=139, true clusters=20
purity majority whole data=0.6466826538768985
	Gibbs sampling successful! Start to saving results.
start doc=3752, end doc=5003
total texts=5003, total clusters=252
	Saving successful!
Batch 5
20000
	6254 documents will be analyze. alpha is 187.62.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=1990
to delete batch 6 5
dict_keys([2, 3, 4, 5, 6])
targetbatchNo 5
len(docsPerBatch) 1251
outlier=60, non-outlier=1191,=maxPredLabel=2035
high_entropy_words=287, total words=1998
outsPerCluster.values(), nonOuts 60 5955
[44, 153, 33, 161, 153, 313, 32, 17, 286, 14, 114, 152, 46, 58, 2, 8, 50, 233, 95, 17, 67, 138, 240, 18, 81, 116, 49, 127, 49, 51, 2, 8, 179, 298, 143, 35, 61, 425, 149, 76, 30, 46, 5, 26, 174, 67, 237, 16, 2, 93, 7, 21, 2, 6, 115, 139, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 72, 53, 2, 2, 59, 92, 2, 2, 2, 9, 4, 2, 2, 2, 41, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 5, 4, 3, 3]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 6
evaluate total texts=1251
homogeneity_score-whole-data:   0.57550751
completeness_score-whole-data:   0.44609950
nmi_score-whole-data:   0.50260738
pred clusters=102, true clusters=20
purity majority whole data=0.6123101518784972
	Gibbs sampling successful! Start to saving results.
start doc=5003, end doc=6254
total texts=6254, total clusters=295
	Saving successful!
Batch 6
20000
	7505 documents will be analyze. alpha is 225.15.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2383
to delete batch 7 6
dict_keys([2, 3, 4, 5, 6, 7])
targetbatchNo 6
len(docsPerBatch) 1251
outlier=80, non-outlier=1171,=maxPredLabel=2422
high_entropy_words=276, total words=1991
outsPerCluster.values(), nonOuts 80 7126
[44, 183, 33, 188, 199, 453, 32, 17, 356, 14, 133, 194, 49, 58, 2, 8, 50, 296, 116, 17, 80, 178, 308, 18, 84, 123, 50, 149, 49, 51, 2, 8, 206, 379, 191, 35, 77, 560, 178, 76, 30, 46, 5, 26, 204, 73, 293, 16, 2, 104, 7, 21, 2, 6, 124, 168, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 79, 68, 2, 2, 59, 124, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 4, 3, 2, 3, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 7
evaluate total texts=1251
homogeneity_score-whole-data:   0.57303751
completeness_score-whole-data:   0.45522600
nmi_score-whole-data:   0.50738273
pred clusters=119, true clusters=20
purity majority whole data=0.6147082334132694
	Gibbs sampling successful! Start to saving results.
start doc=6254, end doc=7505
total texts=7505, total clusters=357
	Saving successful!
Batch 7
20000
	8756 documents will be analyze. alpha is 262.68.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=2789
to delete batch 8 7
dict_keys([2, 3, 4, 5, 6, 7, 8])
targetbatchNo 7
len(docsPerBatch) 1251
outlier=76, non-outlier=1175,=maxPredLabel=2822
high_entropy_words=260, total words=1922
outsPerCluster.values(), nonOuts 76 8301
[44, 205, 33, 217, 228, 599, 32, 17, 372, 14, 146, 251, 49, 58, 2, 8, 50, 338, 140, 17, 101, 220, 397, 18, 84, 139, 50, 171, 49, 51, 2, 8, 218, 488, 247, 35, 96, 678, 217, 76, 30, 46, 5, 26, 209, 97, 347, 16, 2, 111, 7, 21, 2, 6, 135, 193, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 88, 89, 2, 2, 60, 180, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 17, 3, 2, 3, 2, 14, 4, 2, 4, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 8
evaluate total texts=1251
homogeneity_score-whole-data:   0.59086356
completeness_score-whole-data:   0.48013490
nmi_score-whole-data:   0.52977521
pred clusters=109, true clusters=19
purity majority whole data=0.636290967226219
	Gibbs sampling successful! Start to saving results.
start doc=7505, end doc=8756
total texts=8756, total clusters=421
	Saving successful!
Batch 8
20000
	10007 documents will be analyze. alpha is 300.21.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=3231
to delete batch 9 8
dict_keys([2, 3, 4, 5, 6, 7, 8, 9])
targetbatchNo 8
len(docsPerBatch) 1251
outlier=79, non-outlier=1172,=maxPredLabel=3269
high_entropy_words=261, total words=1889
outsPerCluster.values(), nonOuts 79 9473
[44, 224, 33, 232, 254, 736, 32, 17, 384, 14, 164, 333, 49, 58, 2, 8, 50, 385, 174, 17, 122, 256, 502, 18, 84, 150, 50, 175, 49, 51, 2, 8, 222, 556, 309, 35, 115, 815, 260, 76, 30, 46, 5, 26, 213, 126, 411, 16, 2, 114, 7, 21, 2, 6, 142, 207, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 93, 128, 2, 2, 60, 247, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 20, 3, 2, 3, 2, 25, 11, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 9
evaluate total texts=1251
homogeneity_score-whole-data:   0.56869058
completeness_score-whole-data:   0.46766262
nmi_score-whole-data:   0.51325229
pred clusters=113, true clusters=18
purity majority whole data=0.586730615507594
	Gibbs sampling successful! Start to saving results.
start doc=8756, end doc=10007
total texts=10007, total clusters=493
	Saving successful!
Batch 9
20000
	11258 documents will be analyze. alpha is 337.74.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=3748
to delete batch 10 9
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10])
targetbatchNo 9
len(docsPerBatch) 1251
outlier=101, non-outlier=1150,=maxPredLabel=3784
high_entropy_words=276, total words=1891
outsPerCluster.values(), nonOuts 101 10623
[44, 243, 33, 237, 272, 829, 32, 17, 386, 14, 169, 456, 49, 58, 2, 8, 50, 438, 216, 17, 148, 311, 550, 18, 84, 153, 50, 179, 49, 51, 2, 8, 224, 658, 364, 35, 147, 895, 295, 76, 30, 46, 5, 26, 224, 185, 483, 16, 2, 118, 7, 21, 2, 6, 145, 228, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 102, 207, 2, 2, 60, 304, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 43, 17, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 10
evaluate total texts=1251
homogeneity_score-whole-data:   0.53319824
completeness_score-whole-data:   0.42078444
nmi_score-whole-data:   0.47036812
pred clusters=129, true clusters=17
purity majority whole data=0.5715427657873701
	Gibbs sampling successful! Start to saving results.
start doc=10007, end doc=11258
total texts=11258, total clusters=572
	Saving successful!
Batch 10
20000
	12509 documents will be analyze. alpha is 375.27.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=4287
to delete batch 11 10
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
targetbatchNo 10
len(docsPerBatch) 1251
outlier=99, non-outlier=1152,=maxPredLabel=4317
high_entropy_words=257, total words=1918
outsPerCluster.values(), nonOuts 99 11775
[44, 252, 33, 251, 292, 874, 32, 17, 386, 14, 169, 551, 49, 58, 2, 8, 50, 490, 256, 17, 184, 366, 592, 18, 84, 155, 50, 188, 49, 51, 2, 8, 224, 784, 431, 35, 194, 974, 349, 76, 30, 46, 5, 26, 239, 225, 597, 16, 2, 120, 7, 21, 2, 6, 145, 229, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 117, 281, 2, 2, 60, 383, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 56, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 11
evaluate total texts=1251
homogeneity_score-whole-data:   0.53203616
completeness_score-whole-data:   0.42177334
nmi_score-whole-data:   0.47053142
pred clusters=123, true clusters=15
purity majority whole data=0.5715427657873701
	Gibbs sampling successful! Start to saving results.
start doc=11258, end doc=12509
total texts=12509, total clusters=651
	Saving successful!
Batch 11
20000
	13760 documents will be analyze. alpha is 412.80.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=4809
to delete batch 12 11
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])
targetbatchNo 11
len(docsPerBatch) 1251
outlier=80, non-outlier=1171,=maxPredLabel=4834
high_entropy_words=266, total words=1963
outsPerCluster.values(), nonOuts 80 12946
[44, 260, 33, 257, 336, 925, 32, 17, 386, 14, 169, 639, 49, 58, 2, 8, 50, 542, 286, 17, 249, 413, 629, 18, 84, 155, 50, 190, 49, 51, 2, 8, 224, 881, 490, 35, 251, 1071, 401, 76, 30, 46, 5, 26, 256, 302, 688, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 142, 359, 2, 2, 60, 453, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 62, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 12
evaluate total texts=1251
homogeneity_score-whole-data:   0.54934339
completeness_score-whole-data:   0.43578391
nmi_score-whole-data:   0.48601843
pred clusters=105, true clusters=14
purity majority whole data=0.5891286970423661
	Gibbs sampling successful! Start to saving results.
start doc=12509, end doc=13760
total texts=13760, total clusters=722
	Saving successful!
Batch 12
20000
	15011 documents will be analyze. alpha is 450.33.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=5376
to delete batch 13 12
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])
targetbatchNo 12
len(docsPerBatch) 1251
outlier=96, non-outlier=1155,=maxPredLabel=5404
high_entropy_words=260, total words=1882
outsPerCluster.values(), nonOuts 96 14101
[44, 262, 33, 260, 371, 972, 32, 17, 388, 14, 169, 728, 49, 58, 2, 8, 50, 600, 315, 17, 292, 482, 658, 18, 84, 155, 50, 192, 49, 51, 2, 8, 224, 980, 544, 35, 319, 1135, 463, 76, 30, 46, 5, 26, 270, 381, 781, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 180, 436, 2, 2, 60, 535, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 66, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 7, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 13
evaluate total texts=1251
homogeneity_score-whole-data:   0.56825344
completeness_score-whole-data:   0.44636284
nmi_score-whole-data:   0.49998650
pred clusters=117, true clusters=14
purity majority whole data=0.6219024780175859
	Gibbs sampling successful! Start to saving results.
start doc=13760, end doc=15011
total texts=15011, total clusters=804
	Saving successful!
Batch 13
20000
	16262 documents will be analyze. alpha is 487.86.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=5932
to delete batch 14 13
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])
targetbatchNo 13
len(docsPerBatch) 1251
outlier=90, non-outlier=1161,=maxPredLabel=5958
high_entropy_words=274, total words=1906
outsPerCluster.values(), nonOuts 90 15262
[44, 264, 33, 261, 404, 1034, 32, 17, 388, 14, 169, 791, 49, 58, 2, 8, 50, 655, 351, 17, 343, 527, 686, 18, 84, 155, 50, 196, 49, 51, 2, 8, 224, 1081, 593, 35, 384, 1196, 541, 76, 30, 46, 5, 26, 296, 490, 868, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 205, 523, 2, 2, 60, 619, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 68, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 12, 2, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 14
evaluate total texts=1251
homogeneity_score-whole-data:   0.55745654
completeness_score-whole-data:   0.44338928
nmi_score-whole-data:   0.49392273
pred clusters=108, true clusters=14
purity majority whole data=0.6107114308553158
	Gibbs sampling successful! Start to saving results.
start doc=15011, end doc=16262
total texts=16262, total clusters=883
	Saving successful!
Batch 14
20000
	17513 documents will be analyze. alpha is 525.39.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=6419
to delete batch 15 14
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
targetbatchNo 14
len(docsPerBatch) 1251
outlier=73, non-outlier=1178,=maxPredLabel=6444
high_entropy_words=254, total words=1901
outsPerCluster.values(), nonOuts 73 16440
[44, 264, 33, 261, 409, 1109, 32, 17, 388, 14, 169, 833, 49, 58, 2, 8, 50, 674, 366, 17, 428, 556, 730, 18, 84, 155, 50, 196, 49, 51, 2, 8, 224, 1139, 680, 35, 467, 1260, 593, 76, 30, 46, 5, 26, 316, 630, 953, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 237, 637, 2, 2, 60, 723, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 68, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 3, 2, 2, 6, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 15
evaluate total texts=1251
homogeneity_score-whole-data:   0.58934158
completeness_score-whole-data:   0.46851076
nmi_score-whole-data:   0.52202536
pred clusters=97, true clusters=13
purity majority whole data=0.6354916067146283
	Gibbs sampling successful! Start to saving results.
start doc=16262, end doc=17513
total texts=17513, total clusters=950
	Saving successful!
Batch 15
20000
	18764 documents will be analyze. alpha is 562.92.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=6881
to delete batch 16 15
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])
targetbatchNo 15
len(docsPerBatch) 1251
outlier=75, non-outlier=1176,=maxPredLabel=6904
high_entropy_words=222, total words=1853
outsPerCluster.values(), nonOuts 75 17616
[44, 264, 33, 261, 411, 1144, 32, 17, 388, 14, 169, 859, 49, 58, 2, 8, 50, 699, 372, 17, 544, 584, 769, 18, 84, 155, 50, 196, 49, 51, 2, 8, 224, 1176, 721, 35, 593, 1294, 624, 76, 30, 46, 5, 26, 318, 866, 1045, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 329, 727, 2, 2, 60, 833, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 68, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 3, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 16
evaluate total texts=1251
homogeneity_score-whole-data:   0.60651952
completeness_score-whole-data:   0.44956722
nmi_score-whole-data:   0.51638049
pred clusters=94, true clusters=10
purity majority whole data=0.6586730615507594
	Gibbs sampling successful! Start to saving results.
start doc=17513, end doc=18764
total texts=18764, total clusters=1020
	Saving successful!
Batch 16
20000
	20000 documents will be analyze. alpha is 600.00.
	Initialization.
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
maxPredLabel=7360
to delete batch 17 16
dict_keys([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])
targetbatchNo 16
len(docsPerBatch) 1251
outlier=73, non-outlier=1163,=maxPredLabel=7382
high_entropy_words=153, total words=1648
outsPerCluster.values(), nonOuts 73 18779
[44, 264, 33, 261, 411, 1172, 32, 17, 388, 14, 169, 885, 49, 58, 2, 8, 50, 713, 384, 17, 671, 590, 785, 18, 84, 155, 50, 196, 49, 51, 2, 8, 224, 1269, 727, 35, 671, 1322, 656, 76, 30, 46, 5, 26, 320, 1383, 1104, 16, 2, 120, 7, 21, 2, 6, 145, 238, 5, 55, 7, 8, 26, 42, 39, 9, 13, 2, 2, 2, 2, 3, 19, 2, 3, 382, 731, 2, 2, 60, 877, 2, 2, 2, 9, 4, 2, 2, 2, 44, 2, 2, 2, 2, 3, 2, 2, 2, 4, 2, 14, 2, 2, 3, 3, 2, 3, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 6, 4, 5, 3, 2, 3, 24, 3, 2, 3, 2, 68, 20, 2, 6, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 2, 2, 2, 3, 2, 2, 6, 2, 2, 2, 2, 2, 2, 10, 3, 2, 3, 2]
	iter is  1
	iter is  2
	iter is  3
	iter is  4
	iter is  5
	iter is  6
	iter is  7
	iter is  8
	iter is  9
	iter is  10
Evaluate 17
evaluate total texts=1236
homogeneity_score-whole-data:   0.66439170
completeness_score-whole-data:   0.35465458
nmi_score-whole-data:   0.46245114
pred clusters=93, true clusters=6
purity majority whole data=0.8478964401294499
	Gibbs sampling successful! Start to saving results.
start doc=18764, end doc=20000
total texts=20000, total clusters=1083
	Saving successful!
time diff secs= 406
pred_true_text_file name=result/mstr-enh
evaluate total texts=20000
homogeneity_score-whole-data:   0.50741665
completeness_score-whole-data:   0.38481943
nmi_score-whole-data:   0.43769533
pred clusters=1302, true clusters=20
purity majority whole data=0.544
pred_true_text_file name=result/NewsPredTueTextMStream_WordArr.txt
evaluate total texts=20000
homogeneity_score-whole-data:   0.49583550
completeness_score-whole-data:   0.38308666
nmi_score-whole-data:   0.43222932
pred clusters=1071, true clusters=20
purity majority whole data=0.5347
